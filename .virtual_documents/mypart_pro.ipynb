import pandas as pd
from pandas.api.types import CategoricalDtype
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy.stats import chi2_contingency
import statsmodels.api as sm
from statsmodels.formula.api import ols
import warnings


general_ds = pd.read_csv(r"C:\Users\jnnkw\OneDrive\Desktop\pres\general_ds_final.csv", header = 0)


general_ds.info()


general_ds.describe()


# Job distribution
# Plotting the count of Job using seaborn

plt.figure(figsize=(10, 6))
ax = sns.countplot(data=general_ds, x='Job')
plt.title('Count of Job')
plt.xlabel('Job')
plt.ylabel('Count')
plt.xticks(rotation=90)

# Adding numbers on top of the bars
for p in ax.patches:
    ax.annotate(str(int(p.get_height())), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')
plt.show()


# Gender distribution

# Plotting the count of Gender by Job Type using seaborn

plt.figure(figsize=(10, 6))
ax = sns.countplot(data=general_ds, x='Job', hue='Mental_Health_Category')
plt.title('Count of Mental by Job Type')
plt.xlabel('Job Type')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Mental_Health_Category')

# Adding numbers on top of the bars
for p in ax.patches:
    ax.annotate(str(int(p.get_height())), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')
plt.show()


# Gender distribution
# Plotting the count of Gender by Job Type using seaborn

plt.figure(figsize=(10, 6))
ax = sns.countplot(data=general_ds, x='Job', hue='Gender')
plt.title('Count of Gender by Job Type')
plt.xlabel('Job Type')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Gender')

# Adding numbers on top of the bars
for p in ax.patches:
    ax.annotate(str(int(p.get_height())), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')
plt.show()


# Count the occurrences of each category
bmi_counts = general_ds['BMI_Category'].value_counts()

# Plot the pie chart
plt.figure(figsize=(10, 7))
plt.pie(bmi_counts, labels=bmi_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of BMI Category')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()


# Count plot of Ethnicity 

sns.countplot(data=general_ds, x='Ethnicity') 

plt.title('Count of Ethnicity') 

plt.xticks(rotation=90) 

plt.show() 





# Define the columns for diet and shift data
diet_columns = ['Total.fruit.intake_y', 'Total.vegetable.intake_y', 'Water_intake', 
                'Cereal_intake', 'Tea_intake', 'Coffee_intake', 'Cheese_intake', 'total.fish', 
                'Processed_Meat', 'Unprocessed_Meat', 'Bread_intake/week', 'Wholegrain_Bread', 
                'Breakfast_cereal_intake/week', 'Wholegrain_cereal', 
                'Refinedgrain_Bread', 'Refinedgrain_Cereal', 'Milk_type', 'Bread_Type', 'Cereal_Type']


shift_columns = ['Does_Job_Involve_Shift', 'Day_shift_worked', 'Mixture_of_day_and_night_shifts_worked', 
                 'length_of_each_nightshift_during_mixed_shiftperiods', 
                 'Number_of_nightshifts_workedmonthly_during_mixed_shiftperiods', 
                 'Consequetive_nightshifts_during_mixed_shiftperiods']


# Select the data
diet_data = general_ds[diet_columns].dropna()
shift_data = general_ds[shift_columns].dropna()

# Identify categorical columns
diet_categorical = diet_data.select_dtypes(include=['object', 'category']).columns.tolist() 
shift_categorical = shift_data.select_dtypes(include=['object', 'category']).columns.tolist()


#Determining and appending the key shift pattern of the clinicians

kmeans_shift = KMeans(n_clusters=4, n_init=10, random_state=0)
general_ds['Key_Shift_Pattern'] = kmeans_shift.fit_predict(shift_data)

# Print clusters for key shift patterns
print("Key Shift Patterns:")
print(general_ds['Key_Shift_Pattern'].value_counts())


# PCA visualization for shift patterns
scaler = StandardScaler()
shift_data_scaled = scaler.fit_transform(shift_data)

pca = PCA(n_components=2)
shift_pca = pca.fit_transform(shift_data_scaled)

plt.figure(figsize=(10, 8))
scatter = plt.scatter(shift_pca[:, 0], shift_pca[:, 1], c=general_ds['Key_Shift_Pattern'], cmap='viridis')
plt.title('PCA of Shift Patterns')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.colorbar(scatter)
plt.show()


# Analyzing Characteristics of Each shift pattern Cluster 

for cluster in range(4):
    print(f"\nCharacteristics of Shift Pattern {cluster}:")
    cluster_data = shift_data[general_ds['Key_Shift_Pattern'] == cluster]
    print(cluster_data.mean())


# Statistical Tests to Confirm Differences in shift pattern

from scipy import stats

def anova_test(data, labels, variable):
    groups = [data[labels == i][variable] for i in range(max(labels) + 1)]
    f_value, p_value = stats.f_oneway(*groups)
    return f_value, p_value

for column in shift_data.columns:
    f_value, p_value = anova_test(shift_data, general_ds['Key_Shift_Pattern'], column)
    print(f"ANOVA for {column}: F-value = {f_value:.2f}, p-value = {p_value:.4f}")


# Cross-tabulation of shift pattern with Other Variables (Job)

job_shift = pd.crosstab(general_ds['Job'], general_ds['Key_Shift_Pattern'])
print("Job Role vs Shift Pattern:")
print(job_shift)


# Heatmap for Job Role vs Shift Pattern
plt.figure(figsize=(8, 4))
sns.heatmap(job_shift, annot=True, cmap="YlGnBu", fmt="d")
plt.title("Job Role vs Shift Pattern")
plt.xlabel("Shift Pattern")
plt.ylabel("Job Role")
plt.tight_layout()
plt.show()


#Determining and appending the key diet pattern of the clinicians

categorical_columns = ['Milk_type', 'Bread_Type', 'Cereal_Type']
diet_data_encoded = pd.get_dummies(diet_data, columns=categorical_columns)

kmeans_diet = KMeans(n_clusters=5, n_init=10, random_state=0)
general_ds['Key_Diet_Pattern'] = kmeans_diet.fit_predict(diet_data)

# Print clusters for key diet patterns

print("\nKey Diet Patterns:")
print(general_ds['Key_Diet_Pattern'].value_counts())


# Analyzing Characteristics of Each Cluster in the diet pattern

# Assign cluster labels to the original data
diet_data['Cluster'] = kmeans_diet.labels_

# Calculate the mean of each feature for each cluster
cluster_means = diet_data.groupby('Cluster').mean()

print("\nCluster Means:")
print(cluster_means)


# Statistical test to confirm the differences in diet pattern cluster

from scipy import stats

def anova_test(data, labels, variable):
    groups = [data[labels == i][variable] for i in range(max(labels) + 1)]
    f_value, p_value = stats.f_oneway(*groups)
    return f_value, p_value

for column in diet_data.columns:
    f_value, p_value = anova_test(diet_data, general_ds['Key_Diet_Pattern'], column)
    print(f"ANOVA for {column}: F-value = {f_value:.2f}, p-value = {p_value:.4f}")


# Cross-tabulation of the diet pattern with 'Job'

job_diet = pd.crosstab(general_ds['Job'], general_ds['Key_Diet_Pattern'])
print("\nJob Role vs Diet Pattern:")
print(job_diet)


# Heatmap for Job Role vs Diet Pattern
plt.figure(figsize=(12, 8))
sns.heatmap(job_diet, annot=True, cmap="YlGnBu", fmt="d")
plt.title("Job Role vs Diet Pattern")
plt.xlabel("Diet Pattern")
plt.ylabel("Job Role")
plt.tight_layout()
plt.show()


#Determining the association between the key shift pattern and the key diet pattern

contingency_table = pd.crosstab(general_ds['Key_Shift_Pattern'], general_ds['Key_Diet_Pattern'])
print(contingency_table)

# Visualize association between shift and diet patterns
plt.figure(figsize=(12, 8))
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.title('Association between Shift and Diet Patterns')
plt.xlabel('Diet Pattern')
plt.ylabel('Shift Pattern')
plt.show()


#Determining the association of key shift and diet pattern using Chi Square

chi2, p, dof, ex = chi2_contingency(contingency_table)
print(f"Chi2: {chi2}, p-value: {p}")


#Determining the combined association of the Keys shift and diet patterns on Mental health using Anova

anova_shift = ols('Mental_Health_Score ~ C(Key_Shift_Pattern)', data=general_ds).fit()
anova_results_shift = sm.stats.anova_lm(anova_shift)
print(anova_results_shift)

anova_diet = ols('Mental_Health_Score ~ C(Key_Diet_Pattern)', data=general_ds).fit()
anova_results_diet = sm.stats.anova_lm(anova_diet)
print(anova_results_diet)


# Visualize ANOVA association with mental health score
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Key_Shift_Pattern', y='Mental_Health_Score', data=general_ds)
plt.title('Mental Health Score by Shift Pattern')

plt.subplot(1, 2, 2)
sns.boxplot(x='Key_Diet_Pattern', y='Mental_Health_Score', data=general_ds)
plt.title('Mental Health Score by Diet Pattern')

plt.tight_layout()
plt.show()

# Print ANOVA results
print("\nANOVA Results for Shift Patterns:")
print(anova_results_shift)

print("\nANOVA Results for Diet Patterns:")
print(anova_results_diet)


pip install category_encoders


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc

# 1. Data Preprocessing
columns_to_drop = ['eid', 'Overall_LifeStyle_Score', 'Healthy_Diet_Score', 
                   'Key_Shift_Pattern', 'Key_Diet_Pattern', 'Healthy_Sleep_Score_y', 
                   'Healthy_Sleep_Category', 'Mental_Health_Category', 'Cereal_intake', 'Tea_intake', 'Coffee_intake',
                   'Wholegrain_Bread', 'Breakfast_cereal_intake/week', 'Wholegrain_cereal', 'Refinedgrain_Bread', 'Refinedgrain_Cereal']

X = general_ds.drop(columns_to_drop + ['Mental_Health_Score'], axis=1)
y_score = general_ds['Mental_Health_Score']
y_category = general_ds['Mental_Health_Category']

# Handle missing values (if needed)
# X = X.fillna(X.mean())

# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns


# Label Encoding
le = LabelEncoder()
for col in categorical_columns:
    X[col] = le.fit_transform(X[col].astype(str))


#  Feature Selection using Correlation
correlation_matrix = X.corrwith(y_score)
correlation_matrix = correlation_matrix.abs().sort_values(ascending=False)

# Select top K features based on correlation
K = 40  
selected_features = correlation_matrix.head(K).index.tolist()
print("Selected features based on correlation:")
print(selected_features)

# Visualize correlations
plt.figure(figsize=(20, 18))
sns.heatmap(X[selected_features].corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Selected Features')
plt.tight_layout()
plt.show()


# 4. Prepare data for classification
X_final = X[selected_features]
X_train, X_test, y_train, y_test = train_test_split(X_final, y_category, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# 5. Build and train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# 6. Make predictions
y_pred = rf_model.predict(X_test_scaled)
y_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probability of positive class

# 7. Evaluate the model
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

# 8. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()


from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
from itertools import cycle

# 9. ROC Curve for multiclass
n_classes = len(np.unique(y_test))
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
y_pred_proba = rf_model.predict_proba(X_test_scaled)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(10, 8))
colors = cycle(['blue', 'red', 'green', 'yellow', 'purple'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {i} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass')
plt.legend(loc="lower right")
plt.show()


# 10. Feature Importance (this part remains the same)
feature_importance = pd.DataFrame({
    'feature': selected_features,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance)
plt.title('Feature Importance')
plt.tight_layout()
plt.show()

print("\nTop 10 important features:")
print(feature_importance.head(20))


from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier
import numpy as np

rf_model = RandomForestClassifier(random_state=42)

# Perform 5-fold cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(rf_model, X_final, y_category, cv=cv, scoring='accuracy')

print(f"Cross-validation scores: {cv_scores}")
print(f"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")





from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a base model
rf_model = RandomForestClassifier(random_state=42)

# Instantiate the grid search model
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, 
                           cv=5, n_jobs=4, verbose=2, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_final, y_category)



# Fit the grid search to the data
grid_search.fit(X_final, y_category)

# Print the best parameters and score
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")


# Get the best model
best_model = grid_search.best_estimator_

# Perform cross-validation on the best model
best_cv_scores = cross_val_score(best_model, X_final, y_category, cv=5, scoring='accuracy')

print(f"Best model cross-validation scores: {best_cv_scores}")
print(f"Best model mean CV accuracy: {best_cv_scores.mean():.4f} (+/- {best_cv_scores.std() * 2:.4f})")



